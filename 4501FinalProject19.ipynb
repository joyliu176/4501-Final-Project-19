{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce6f89c",
   "metadata": {},
   "source": [
    "<center><h1><b>NYC Apartment Search</b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5a4e6",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n",
    "\n",
    "## Import Statements\n",
    "\n",
    "This code block includes various import statements that bring in libraries and modules necessary for data manipulation, visualization, database operations, and geographic information system (GIS) tasks. These imports cover a wide range of functionalities required for the project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import unittest\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Database and GIS related imports\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import geoalchemy2 as gdb\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon, mapping\n",
    "from shapely import wkb\n",
    "\n",
    "# SQLAlchemy imports\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    Integer,\n",
    "    Float,\n",
    "    String,\n",
    "    DateTime,\n",
    "    text,\n",
    "    ForeignKey,\n",
    ")\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.schema import CreateTable\n",
    "from geoalchemy2 import Geometry\n",
    "from geoalchemy2.shape import to_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcaa2c",
   "metadata": {},
   "source": [
    "## Constants and File Paths\n",
    "\n",
    "This code block defines several constants and file paths for data directories and files related to the project. It also includes constants for accessing New York City data through an API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a551e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and directory constants\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ZIPCODE_DATA_FILE = DATA_DIR / \"zipcodes\" / \"nyc_zipcodes.shp\"\n",
    "ZILLOW_DATA_FILE = DATA_DIR / \"zillow_rent_data.csv\"\n",
    "\n",
    "# New York City data API constants\n",
    "NYC_DATA_APP_TOKEN = \"SMg9akfNT3gV1L4QAEb8vlx4F\"\n",
    "BASE_NYC_DATA_URL = \"https://data.cityofnewyork.us/\"\n",
    "NYC_DATA_311 = \"erm2-nwe9.geojson\"\n",
    "NYC_DATA_TREES = \"5rq2-4hqu.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database constants\n",
    "DB_NAME = \"group19project\"\n",
    "DB_USER = \"cecilialin\"\n",
    "DB_URL = f\"postgres+psycopg2://{DB_USER}@localhost/{DB_NAME}\"\n",
    "DB_SCHEMA_FILE = \"schema.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for DB queries for Part 3\n",
    "QUERY_DIR = pathlib.Path(\"queries\")\n",
    "\n",
    "# Make sure the QUERY_DIRECTORY exists\n",
    "if not QUERY_DIR.exists():\n",
    "    QUERY_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ed075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path\n",
    "directory = \"data/resource\"\n",
    "\n",
    "# Create the directory. exist_ok=True means it won't throw an error if the directory already exists.\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Directory {directory} has been created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0baf20f",
   "metadata": {},
   "source": [
    "## Logging Setup\n",
    "\n",
    "This code block sets up logging for information (INFO) and error tracking. It configures the logging module to handle log messages for the current module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2572ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging for info and error tracking\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8af43",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Data Preprocessing\n",
    "\n",
    "In Part 1 of the project, the primary focus is on data preprocessing. This stage involves downloading datasets, both manually and programmatically, cleaning and filtering the data, filling in missing values, and generating relevant data samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b099b",
   "metadata": {},
   "source": [
    "## New York City GeoJSON Data Downloading\n",
    "\n",
    "This function downloads New York City GeoJSON data in batches and saves them to individual files. Key features include:\n",
    "\n",
    "- **Batch Downloading**: Downloads data in manageable batches, controlled by the `limit` parameter.\n",
    "- **File Saving**: Each batch is saved as a separate GeoJSON file for efficient data handling.\n",
    "- **Error Handling and Logging**: Incorporates error handling for robust data retrieval and logs progress for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_nyc_geojson_data(url: str, limit: int = 1000000) -> List[pathlib.Path]:\n",
    "    \"\"\"\n",
    "    Downloads NYC GeoJSON data in batches and saves each batch to a separate file.\n",
    "\n",
    "    :param url: URL to download the data from.\n",
    "    :param limit: Number of records per batch to download (default is 1,000,000).\n",
    "    :return: List of file paths of the downloaded GeoJSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_url = urllib.parse.urlparse(url)\n",
    "    url_path = parsed_url.path.strip(\"/\")\n",
    "    total_record_count = 0\n",
    "    file_index = 0\n",
    "    more_data_available = True\n",
    "\n",
    "    while more_data_available:\n",
    "        current_filename = DATA_DIR / f\"{url_path}_{file_index}.geojson\"\n",
    "        if not current_filename.exists():\n",
    "            logger.info(f\"Downloading data to {current_filename}...\")\n",
    "            try:\n",
    "                params = {'$limit': limit, '$offset': file_index * limit}\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                if data:\n",
    "                    with open(current_filename, \"w\") as f:\n",
    "                        json.dump(data, f)\n",
    "                    total_record_count += len(data)\n",
    "\n",
    "                    # Check if the returned data is less than the requested limit\n",
    "                    if len(data) < limit:\n",
    "                        more_data_available = False\n",
    "                    else:\n",
    "                        file_index += 1\n",
    "\n",
    "                else:\n",
    "                    more_data_available = False\n",
    "            except requests.RequestException as e:\n",
    "                logger.error(f\"Failed to retrieve data: {e}\")\n",
    "                break\n",
    "        else:\n",
    "            logger.info(f\"File {current_filename} already exists. Skipping download.\")\n",
    "            file_index += 1\n",
    "\n",
    "    logger.info(f\"Total records downloaded: {total_record_count}\")\n",
    "    return [DATA_DIR / f\"{url_path}_{i}.geojson\" for i in range(file_index)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bdd9d",
   "metadata": {},
   "source": [
    "## Data Query and Download Process\n",
    "\n",
    "This section of the notebook sets up and executes the data download process for two specific datasets: NYC 311 Service Requests and the 2015 Street Tree Census. The steps include:\n",
    "\n",
    "1. **Setting Date Range**: Defines the start and end dates for the data query. This range is used to filter the data to a specific time period.\n",
    "   \n",
    "2. **Constructing Download URLs**: Prepares URLs for the data API with the specified date range and includes the application token for authentication.\n",
    "\n",
    "3. **Downloading Data**:\n",
    "   - For the 311 Service Requests, the constructed URL is used to download data and save it to a list of files.\n",
    "   - Similarly, for the Street Tree Census, data for the year 2015 is downloaded using its specific URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end date for data query\n",
    "start_date = datetime(2015, 1, 1)\n",
    "end_date = datetime.now()\n",
    "date_query = f\"$where=created_date between '{start_date.isoformat()}' and '{end_date.isoformat()}'\"\n",
    "\n",
    "# Construct the full URL for the 311 data download\n",
    "url_311 = f\"{BASE_NYC_DATA_URL}resource/{NYC_DATA_311}?{date_query}&$$app_token={NYC_DATA_APP_TOKEN}\"# Download and merge 311 Service Requests Data\n",
    "file_list_311 = download_nyc_geojson_data(url_311)\n",
    "file_list_311\n",
    "\n",
    "# Define URL for 2015 Street Tree Census data\n",
    "url_2015 = f\"{BASE_NYC_DATA_URL}resource/{NYC_DATA_TREES}?&$$app_token={NYC_DATA_APP_TOKEN}\"\n",
    "file_list_2015 = download_nyc_geojson_data(url_2015)\n",
    "file_list_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b55c9",
   "metadata": {},
   "source": [
    "## Zipcode Data\n",
    "\n",
    "This function is responsible for loading, cleaning, and normalizing the zipcode data. The process includes:\n",
    "\n",
    "1. **Data Loading**: Reads the zipcode data from a file into a GeoDataFrame.\n",
    "2. **Column Selection**: Retains only essential columns - 'ZIPCODE' and 'geometry'.\n",
    "3. **Data Cleaning**: Removes any rows with missing values.\n",
    "4. **Normalization**: \n",
    "   - Standardizes column names to lowercase.\n",
    "   - Converts the 'zipcode' column to string format.\n",
    "   - Sets the geometry column to a consistent Spatial Reference Identifier (SRID) for geographic consistency.\n",
    "5. **Polygon Dissolving**: Dissolves multiple polygons into a single polygon per zipcode.\n",
    "6. **Index Resetting**: Resets the index to include 'zipcode' as a regular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff856965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zipcodes(zipcode_datafile):\n",
    "    # Load data\n",
    "    gdf = gpd.read_file(zipcode_datafile)\n",
    "\n",
    "    # Keep only necessary columns\n",
    "    columns_needed = ['ZIPCODE', 'geometry']\n",
    "    gdf = gdf[columns_needed]\n",
    "\n",
    "    # Remove invalid data points\n",
    "    gdf.dropna(inplace=True)  # Detailed cleaning as per specific situations\n",
    "\n",
    "    # Normalize column names (to lowercase)\n",
    "    gdf.columns = [col.lower() for col in gdf.columns]\n",
    "\n",
    "    # Normalize data types\n",
    "    gdf['zipcode'] = gdf['zipcode'].astype(str)\n",
    "\n",
    "    # Normalize the Spatial Reference Identifier (SRID) of the geometry\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # Dissolve polygons by zipcode\n",
    "    gdf_dissolved = gdf.dissolve(by='zipcode')\n",
    "\n",
    "    # Reset index to turn the 'zipcode' index into a column\n",
    "    gdf_dissolved.reset_index(inplace=True)\n",
    "\n",
    "    return gdf_dissolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function\n",
    "zipcode_datafile = ZIPCODE_DATA_FILE\n",
    "cleaned_gdf = load_and_clean_zipcodes(zipcode_datafile)\n",
    "print(cleaned_gdf.head())\n",
    "cleaned_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f8ffa6",
   "metadata": {},
   "source": [
    "## Complaints Data\n",
    "\n",
    "### `clean_and_save_311_file`\n",
    "Cleans and saves a single 311 dataset GeoJSON file. Steps include file validation, data loading, column normalization, spatial join with NYC zip codes, and saving the cleaned data.\n",
    "\n",
    "### `load_and_clean_311_data`\n",
    "Processes a list of 311 data files using `clean_and_save_311_file`, streamlining the cleaning and saving process for multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defb4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_311_file(file_path, output_directory):\n",
    "    \"\"\"\n",
    "    Clean a single GeoJSON file and save the cleaned data to the specified output directory.\n",
    "\n",
    "    :param file_path: File path of the GeoJSON file to clean.\n",
    "    :param output_directory: Directory to save the cleaned GeoJSON files.\n",
    "    \"\"\"\n",
    "    # Construct the output file path\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        logging.warning(f\"Output file already exists: {output_file_path}\")\n",
    "        return\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Log the processing of the file\n",
    "    logging.info(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # Create the output directory if it does not exist\n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load the GeoJSON file\n",
    "    gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # Perform cleaning operations\n",
    "    # Load and clean NYC zipcode data\n",
    "    nyc_zipcodes_gdf = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "\n",
    "    # Clean and preprocess 311 data\n",
    "    columns_needed = ['unique_key', 'created_date', 'complaint_type', 'incident_zip', 'geometry']\n",
    "    gdf = gdf[columns_needed]\n",
    "\n",
    "    # Remove duplicates\n",
    "    gdf.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Rename column 'incident_zip' to 'zipcode'\n",
    "    gdf.rename(columns={'incident_zip': 'zipcode'}, inplace=True)\n",
    "\n",
    "    # Normalize column names & column types\n",
    "    gdf.columns = [col.lower().replace(' ', '_') for col in gdf.columns]\n",
    "    gdf['unique_key'] = gdf['unique_key'].astype(str)\n",
    "    gdf['created_date'] = pd.to_datetime(gdf['created_date'], errors='coerce').dropna()\n",
    "    gdf['zipcode'] = gdf['zipcode'].astype(str)\n",
    "\n",
    "    # Normalize the Spatial Reference Identifier (SRID) of the geometry\n",
    "    gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    # Ensure the 'zipcode' column in both dataframes is of the same type\n",
    "    gdf['zipcode'] = gdf['zipcode'].astype(str)\n",
    "    nyc_zipcodes_gdf['zipcode'] = nyc_zipcodes_gdf['zipcode'].astype(str)\n",
    "\n",
    "    # Keep only data points whose zipcode exists in NYC zipcode dataframe\n",
    "    gdf = gdf[gdf['zipcode'].isin(nyc_zipcodes_gdf['zipcode'])]\n",
    "    gdf = gpd.sjoin(gdf, nyc_zipcodes_gdf, how='inner', predicate='intersects')\n",
    "    \n",
    "    # Merge 'zipcode_left' and 'zipcode_right' columns into a single 'zipcode' column\n",
    "    gdf['zipcode'] = gdf['zipcode_left'].fillna(gdf['zipcode_right']).astype(str)\n",
    "    \n",
    "    # Drop the 'zipcode_left' and 'zipcode_right' columns\n",
    "    gdf.drop(['zipcode_left', 'zipcode_right'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop the 'index_right' column\n",
    "    gdf.drop(['index_right'], axis=1, inplace=True)\n",
    "\n",
    "    # Save the cleaned GeoDataFrame as a new GeoJSON file\n",
    "    gdf.to_file(output_file_path, driver='GeoJSON')\n",
    "\n",
    "    # Log completion message\n",
    "    logging.info(f\"File processed and saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fe3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_311_data(file_list, output_directory):\n",
    "    \"\"\"\n",
    "    Process each file in the file list: clean and save to the specified output directory.\n",
    "\n",
    "    :param file_list: List of file paths of GeoJSON files to process.\n",
    "    :param output_directory: Directory to save the cleaned GeoJSON files.\n",
    "    \"\"\"\n",
    "    for file_path in file_list:\n",
    "        clean_and_save_311_file(file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_clean_311_data(file_list_311, 'data/cleaned_resource')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d452f4",
   "metadata": {},
   "source": [
    "## Tree Data\n",
    "\n",
    "### `clean_and_save_tree_data`\n",
    "Cleans a single tree data file by removing non-NYC entries, duplicate tree IDs, and saves the cleaned data. Involves file verification, data loading, necessary column filtering, spatial join with NYC zip codes, duplicate removal, and saving the cleaned data.\n",
    "\n",
    "### `load_and_clean_tree_data`\n",
    "Processes multiple tree data files, applying `clean_and_save_tree_data` to each file for efficient batch cleaning and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf64360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_save_tree_data(file_path, output_directory):\n",
    "    \"\"\"\n",
    "    Clean a single tree data file, remove entries not in NYC, remove duplicate tree_ids, and save the cleaned data to the specified output directory.\n",
    "\n",
    "    :param file_path: File path of the tree data file to clean.\n",
    "    :param output_directory: Directory to save the cleaned tree data files.\n",
    "    \"\"\"\n",
    "    # Construct the output file path\n",
    "    output_file_path = os.path.join(output_directory, os.path.basename(file_path))\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        logging.warning(f\"Output file already exists: {output_file_path}\")\n",
    "        return\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File does not exist: {file_path}\")\n",
    "        return\n",
    "\n",
    "    # Log the processing of the file\n",
    "    logging.info(f\"Processing file: {file_path}\")\n",
    "\n",
    "    # Create the output directory if it does not exist\n",
    "    Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load the tree data file\n",
    "    gdf = gpd.read_file(file_path)\n",
    "\n",
    "    # Perform cleaning operations\n",
    "    columns_needed = ['tree_id', 'zipcode', 'spc_common', 'health', 'status', 'geometry']\n",
    "    gdf = gdf[columns_needed]\n",
    "\n",
    "    # Removing invalid data points\n",
    "    gdf.dropna(subset=['zipcode', 'health', 'status'], inplace=True)\n",
    "\n",
    "    # Rename 'spc_common' column to 'species'\n",
    "    gdf.rename(columns={'spc_common': 'species'}, inplace=True)\n",
    "\n",
    "    # Normalize column names & column types\n",
    "    gdf.columns = [col.lower().replace(' ', '_') for col in gdf.columns]\n",
    "    gdf['tree_id'] = gdf['tree_id'].astype(str)\n",
    "    gdf['zipcode'] = gdf['zipcode'].astype(str)\n",
    "\n",
    "    # Normalize the Spatial Reference Identifier (SRID) of the geometry\n",
    "    gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    # Load and clean NYC zipcode data\n",
    "    nyc_zipcodes_gdf = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "\n",
    "    # Filter out data points not in NYC\n",
    "    gdf = gdf[gdf['zipcode'].isin(nyc_zipcodes_gdf['zipcode'])]\n",
    "    gdf = gpd.sjoin(gdf, nyc_zipcodes_gdf, how='inner', predicate='intersects')\n",
    "    \n",
    "    # Merge 'zipcode_left' and 'zipcode_right' columns into a single 'zipcode' column\n",
    "    gdf['zipcode'] = gdf['zipcode_left'].fillna(gdf['zipcode_right']).astype(str)\n",
    "    \n",
    "    # Drop the 'zipcode_left' and 'zipcode_right' columns\n",
    "    gdf.drop(['zipcode_left', 'zipcode_right'], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop the 'index_right' column\n",
    "    gdf.drop(['index_right'], axis=1, inplace=True)\n",
    "    \n",
    "    # Remove duplicates based on tree_id\n",
    "    gdf.drop_duplicates(subset=['tree_id'], inplace=True)\n",
    "\n",
    "    # Save the cleaned GeoDataFrame as a new file\n",
    "    gdf.to_file(output_file_path, driver='GeoJSON')\n",
    "\n",
    "    # Log completion message\n",
    "    logging.info(f\"File processed and saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_tree_data(file_list, output_directory):\n",
    "    \"\"\"\n",
    "    Process each file in the file list: clean and save to the specified output directory.\n",
    "\n",
    "    :param file_list: List of file paths of GeoJSON files to process.\n",
    "    :param output_directory: Directory to save the cleaned GeoJSON files.\n",
    "    \"\"\"\n",
    "    for file_path in file_list:\n",
    "        clean_and_save_tree_data(file_path, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8868ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'data/cleaned_resource'\n",
    "load_and_clean_tree_data(file_list_2015, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e6136",
   "metadata": {},
   "source": [
    "## Zillow Data\n",
    "\n",
    "This function is designed to load and clean Zillow rent data, specifically focusing on New York City zip codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7883db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_zillow_data(zillow_datafile):\n",
    "    \"\"\"\n",
    "    Load and clean Zillow rent data, filter for NYC zip codes.\n",
    "\n",
    "    Args:\n",
    "        zillow_datafile (str): Path to the Zillow rent data CSV file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Cleaned and filtered DataFrame containing Zillow rent data for NYC.\n",
    "    \"\"\"\n",
    "    # Load Zillow rent data into a DataFrame\n",
    "    df = pd.read_csv(zillow_datafile)\n",
    "    \n",
    "    # Filter to keep only NYC zip codes\n",
    "    nyc_zipcodes_gdf = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    nyc_zipcodes_gdf['zipcode'] = nyc_zipcodes_gdf['zipcode'].astype(str)\n",
    "    df['RegionName'] = df['RegionName'].astype(str)\n",
    "    df = df[df['RegionName'].isin(nyc_zipcodes_gdf['zipcode'].astype(str))]\n",
    "\n",
    "    # Define columns needed (RegionName and dates columns)\n",
    "    columns_needed = ['RegionName'] + list(df.columns[10:])  # Select columns from 2015/1/31 onwards\n",
    "\n",
    "    # Keep only the necessary columns\n",
    "    df = df[columns_needed]\n",
    "\n",
    "    # Rename 'RegionName' to 'zipcode'\n",
    "    df.rename(columns={'RegionName': 'zipcode'}, inplace=True)\n",
    "\n",
    "    # Normalize column names to lowercase\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "    # Melt the DataFrame to have a structured format (Date, Zipcode, Value)\n",
    "    df = df.melt(id_vars=['zipcode'], var_name='date', value_name='value')\n",
    "\n",
    "    # Convert the 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "    \n",
    "    # Remove rows where 'value' is missing\n",
    "    df = df.dropna(subset=['value'])\n",
    "\n",
    "    # Reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_datafile = ZILLOW_DATA_FILE\n",
    "cleaned_df_zillow = load_and_clean_zillow_data(zillow_datafile)\n",
    "print(cleaned_df_zillow.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07275361",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7446e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load all required data for the project.\n",
    "def load_all_data():\n",
    "    \"\"\"\n",
    "    This function loads and cleans various datasets required for the project.\n",
    "\n",
    "    It loads zip code data, tree data, 311 data, and Zillow data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing four dataframes -\n",
    "           (geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data)\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting to load data...\")\n",
    "\n",
    "    # Load and clean zip code data\n",
    "    geodf_zipcode_data = load_and_clean_zipcodes(ZIPCODE_DATA_FILE)\n",
    "    logger.info(\"Zip code data loaded successfully.\")\n",
    "\n",
    "    # Load and merge tree data\n",
    "    geodf_tree_data_list = []\n",
    "    for j in range(7):\n",
    "        try:\n",
    "            file_path = f\"data/cleaned_resource/5rq2-4hqu.geojson_{j}.geojson\"\n",
    "            geodf_tree_data = gpd.read_file(file_path)\n",
    "            geodf_tree_data_list.append(geodf_tree_data)\n",
    "            logger.info(f\"File loaded: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    geodf_tree_data_j = pd.concat(geodf_tree_data_list, ignore_index=True)\n",
    "    logger.info(\"Tree data loaded and merged successfully.\")\n",
    "\n",
    "    # Load and merge 311 data\n",
    "    geodf_311_data_list = []\n",
    "    for i in range(250):\n",
    "        try:\n",
    "            file_path = f\"data/cleaned_resource/erm2-nwe9.geojson_{i}.geojson\"\n",
    "            geodf_311_data = gpd.read_file(file_path)\n",
    "            geodf_311_data_list.append(geodf_311_data)\n",
    "            logger.info(f\"File loaded: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "    geodf_311_data_i = pd.concat(geodf_311_data_list, ignore_index=True)\n",
    "    logger.info(\"311 data loaded and merged successfully.\")\n",
    "\n",
    "    # Load and clean Zillow data\n",
    "    df_zillow_data = load_and_clean_zillow_data(ZILLOW_DATA_FILE)\n",
    "    logger.info(\"Zillow data loaded successfully.\")\n",
    "\n",
    "    return (\n",
    "        geodf_zipcode_data,\n",
    "        geodf_311_data_i,\n",
    "        geodf_tree_data_j,\n",
    "        df_zillow_data\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess all required data for the project\n",
    "geodf_zipcode_data, geodf_311_data, geodf_tree_data, df_zillow_data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec1285",
   "metadata": {},
   "source": [
    "## Display DataFrame Information and First 5 Entries\n",
    "\n",
    "In this code block, we intend to provide an overview of the data in different DataFrames. The code uses the `info()` method to display basic information about each DataFrame, including the data types and non-null counts of columns. Additionally, the `head()` method is used to display the first 5 entries (rows) of each DataFrame, giving a glimpse of the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9308adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info about each dataframe\n",
    "geodf_zipcode_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee85a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 5 entries about each dataframe\n",
    "geodf_zipcode_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed13a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_311_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf_tree_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f03291",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zillow_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a21abc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Storing Data\n",
    "\n",
    "In Part 2 of the project, the focus shifts to storing the datasets obtained and cleaned in Part 1. The primary objective is to populate a PostgreSQL database with tables generated from these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5daadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!createdb group19project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql --dbname group19project -c 'CREATE EXTENSION postgis;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ffb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PostgreSQL engine\n",
    "engine = create_engine(\"postgresql+psycopg2://joyliu@localhost/group19project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import MetaData\n",
    "from sqlalchemy.orm import declarative_base\n",
    "\n",
    "# Create a metadata object\n",
    "metadata = MetaData()\n",
    "\n",
    "# Define the declarative base class using the updated import path\n",
    "Base = declarative_base(metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685c113",
   "metadata": {},
   "source": [
    "## Database Table Definitions\n",
    "\n",
    "In this section, we define the structure and relationships of database tables that will be used for storing and organizing data related to zip codes, complaints, trees, and rent history. These tables are crucial for the analysis and management of diverse datasets in the project.\n",
    "\n",
    "### ZipCode Table\n",
    "\n",
    "The **ZipCode** table represents geographical information about zip codes. It includes the following fields:\n",
    "\n",
    "- **zipcode**: The primary key, representing the zip code.\n",
    "- **geometry**: A multi-polygon geometry associated with the zip code.\n",
    "\n",
    "This table establishes key relationships with other tables:\n",
    "- **complaints**: A one-to-many relationship with the 'Complaint' table.\n",
    "- **trees**: A one-to-many relationship with the 'Tree' table.\n",
    "- **rent_history**: A one-to-many relationship with the 'RentHistory' table.\n",
    "\n",
    "### Complaint Table\n",
    "\n",
    "The **Complaint** table represents records of complaints, including details about their creation and location. Key fields include:\n",
    "\n",
    "- **id**: The primary key representing the complaint ID.\n",
    "- **created_date**: The date when the complaint was created.\n",
    "- **complaint_type**: The type of complaint (assumed to reference another table).\n",
    "- **zipcode**: A foreign key referencing the 'ZipCode' table for zip code information.\n",
    "- **geometry**: A point geometry associated with the complaint location.\n",
    "\n",
    "This table establishes a many-to-one relationship with the 'ZipCode' table.\n",
    "\n",
    "### Tree Table\n",
    "\n",
    "The **Tree** table stores information about trees, including their species, health status, and location. Essential fields include:\n",
    "\n",
    "- **tree_id**: The primary key representing the tree ID.\n",
    "- **zipcode**: A foreign key referencing the 'ZipCode' table for zip code information.\n",
    "- **species**: The species of the tree.\n",
    "- **health**: The health status of the tree.\n",
    "- **status**: The status of the tree.\n",
    "- **geometry**: A point geometry associated with the tree location.\n",
    "\n",
    "This table also establishes a many-to-one relationship with the 'ZipCode' table.\n",
    "\n",
    "### RentHistory Table\n",
    "\n",
    "The **RentHistory** table records historical data related to rent values. It contains the following fields:\n",
    "\n",
    "- **id**: The primary key representing the rent history record ID.\n",
    "- **zipcode**: A foreign key referencing the 'ZipCode' table for zip code information.\n",
    "- **date**: The date associated with the rent history record.\n",
    "- **value**: The rent value.\n",
    "\n",
    "Similar to other tables, the 'RentHistory' table establishes a many-to-one relationship with the 'ZipCode' table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08dd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ZipCode class to map to the 'zip_codes' table in the database\n",
    "class ZipCode(Base):\n",
    "    __tablename__ = 'zip_codes'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    zipcode = Column(String(5), primary_key=True)  \n",
    "    geometry = Column(Geometry('MULTIPOLYGON', srid=4326))\n",
    "    \n",
    "    # Define relationships with other tables\n",
    "    complaints = relationship('Complaint', back_populates='zip_code')\n",
    "    trees = relationship('Tree', back_populates='zip_code')\n",
    "    rent_history = relationship('RentHistory', back_populates='zip_code')\n",
    "\n",
    "# Define the Complaint class to map to the 'complaints' table in the database\n",
    "class Complaint(Base):\n",
    "    __tablename__ = 'complaints'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    \n",
    "    id = Column(Integer, primary_key=True)\n",
    "    created_date = Column(DateTime)\n",
    "    complaint_type = Column(String)  # Assuming this column references another table\n",
    "    zipcode = Column(String(5), ForeignKey('zip_codes.zipcode'))  # Reference the 'zipcode' column\n",
    "    geometry = Column(Geometry('POINT', srid=4326))\n",
    "    \n",
    "    # Define many-to-one relationship with ZipCode table\n",
    "    zip_code = relationship('ZipCode', back_populates='complaints')\n",
    "\n",
    "# Define the Tree class to map to the 'trees' table in the database\n",
    "class Tree(Base):\n",
    "    __tablename__ = 'trees'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    tree_id = Column(Integer, primary_key=True)\n",
    "    zipcode = Column(String(5), ForeignKey('zip_codes.zipcode'))  # Reference the 'zipcode' column\n",
    "    species = Column(String)\n",
    "    health = Column(String)\n",
    "    status = Column(String)\n",
    "    geometry = Column(Geometry('POINT', srid=4326))\n",
    "    \n",
    "    # Define many-to-one relationship with ZipCode table\n",
    "    zip_code = relationship('ZipCode', back_populates='trees')\n",
    "\n",
    "# Define the RentHistory class to map to the 'rent_history' table in the database\n",
    "class RentHistory(Base):\n",
    "    __tablename__ = 'rent_history'\n",
    "    __table_args__ = {'extend_existing': True}\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    zipcode = Column(String(5), ForeignKey('zip_codes.zipcode'))  # Reference the 'zipcode' column\n",
    "    date = Column(DateTime)\n",
    "    value = Column(Float)\n",
    "    \n",
    "    # Define many-to-one relationship with ZipCode table\n",
    "    zip_code = relationship('ZipCode', back_populates='rent_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables in the database\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329784e",
   "metadata": {},
   "source": [
    "## Generate SQL Schema File\n",
    "\n",
    "This code generates an SQL schema file ('schema.sql') that contains the CREATE TABLE statements for all tables defined in the database metadata and uses the SQLAlchemy library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('schema.sql', 'w') as schema_file:\n",
    "    for table in metadata.sorted_tables:\n",
    "        # Compile the CREATE TABLE statement for the current table using the SQLAlchemy dialect\n",
    "        create_statement = CreateTable(table).compile(dialect=engine.dialect)\n",
    "        \n",
    "        # Write the CREATE TABLE statement to the schema file with a semicolon and newline\n",
    "        schema_file.write(str(create_statement) + ';\\n')\n",
    "\n",
    "# Print a confirmation message\n",
    "print(\"Schema file 'schema.sql' has been generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fb9959",
   "metadata": {},
   "source": [
    "## insert_data Function\n",
    "\n",
    "The `insert_data` function is responsible for inserting data into a PostgreSQL database table. It accepts the following parameters:\n",
    "\n",
    "- `cursor`: The database cursor for executing SQL commands.\n",
    "- `table_name`: The name of the target table in the database.\n",
    "- `data`: A dictionary containing the data to be inserted.\n",
    "- `connection`: The database connection.\n",
    "\n",
    "The function constructs and executes an SQL INSERT statement based on the provided parameters. In case of an error during insertion, it prints detailed error information, including the error message and code. The transaction is rolled back to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0fd17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the insert_data function\n",
    "def insert_data(cursor, table_name, data, connection):\n",
    "    \"\"\"\n",
    "    Inserts data into a PostgreSQL database table, handling potential errors.\n",
    "    \n",
    "    This function constructs and executes an SQL INSERT statement for a specified table in the PostgreSQL database.\n",
    "    It accepts the cursor, target table name, data dictionary, and connection as parameters.\n",
    "    \n",
    "    In case of an error, detailed error information is printed, including the error message and code.\n",
    "    The transaction is rolled back to maintain data integrity.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        columns = ', '.join(data.keys())\n",
    "        placeholders = ', '.join(['%s' for _ in data])\n",
    "        sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "        cursor.execute(sql, tuple(data.values()))\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error inserting data:\", e)\n",
    "        print(\"Error details:\", e.pgerror)  # Print detailed error information\n",
    "        print(\"Error code:\", e.pgcode)     # Print error code\n",
    "        connection.rollback()  # Rollback the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a database connection\n",
    "conn = psycopg2.connect(dbname=\"group19project\", user=\"cecilialin\", host=\"localhost\", port=5432)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03363bf9",
   "metadata": {},
   "source": [
    "### Inserting Geospatial Data\n",
    "\n",
    "This code block processes geospatial data from `geodf_zipcode_data` and inserts it into the 'zip_codes' table in the database. It iterates through the rows, converting geometry to GeoJSON format, and handles progress logging. In case of a database error, it rolls back the transaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process geodf_zipcode_data\n",
    "    for index, row in geodf_zipcode_data.iterrows():\n",
    "        geom_geojson = json.dumps(mapping(row['geometry'])) if row['geometry'] else None\n",
    "        data = {\n",
    "            'zipcode': row['zipcode'],\n",
    "            'geometry': geom_geojson\n",
    "        }\n",
    "        insert_data(cursor, 'zip_codes', data, conn)\n",
    "\n",
    "        # Log progress for each row or at specific intervals\n",
    "        if (index + 1) % 100 == 0 or (index + 1) == geodf_zipcode_data.shape[0]:\n",
    "            logging.info(f\"Inserted ZipCode row {index + 1} of {geodf_zipcode_data.shape[0]}\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Database error:\", e)\n",
    "    conn.rollback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a3369",
   "metadata": {},
   "source": [
    "### Inserting Zillow Rent Data\n",
    "\n",
    "This code block processes data from `df_zillow_data` and inserts it into the 'rent_history' table in the database. It logs progress at specified intervals or for the last row and handles database errors by rolling back the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process df_zillow_data\n",
    "    for index, row in df_zillow_data.iterrows():\n",
    "        data = {\n",
    "            'zipcode': row['zipcode'],\n",
    "            'date': row['date'],\n",
    "            'value': row['value']\n",
    "        }\n",
    "        insert_data(cursor, 'rent_history', data, conn)\n",
    "\n",
    "        # Log progress at specific intervals or for the last row\n",
    "        if (index + 1) % 1000 == 0 or (index + 1) == df_zillow_data.shape[0]:\n",
    "            logging.info(f\"Inserted Rent row {index + 1} of {df_zillow_data.shape[0]}\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Database error:\", e)\n",
    "    conn.rollback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8232827e",
   "metadata": {},
   "source": [
    "### Inserting Tree Data\n",
    "\n",
    "This code processes data from `geodf_tree_data` and inserts it into the 'trees' table in the database. It logs progress at specific intervals or for the last row and handles database errors by rolling back the transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process geodf_tree_data\n",
    "    for index, row in geodf_tree_data.iterrows():\n",
    "        geom_geojson = json.dumps(mapping(row['geometry'])) if row['geometry'] else None\n",
    "        data = {\n",
    "            'tree_id': row['tree_id'],\n",
    "            'zipcode': row['zipcode'],\n",
    "            'species': row['species'],\n",
    "            'health': row['health'],\n",
    "            'status': row['status'],\n",
    "            'geometry': geom_geojson\n",
    "        }\n",
    "        insert_data(cursor, 'trees', data, conn)\n",
    "\n",
    "        # Log progress at specific intervals or for the last row\n",
    "        if (index + 1) % 100000 == 0 or (index + 1) == geodf_tree_data.shape[0]:\n",
    "            logging.info(f\"Inserted Tree row {index + 1} of {geodf_tree_data.shape[0]}\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Database error:\", e)\n",
    "    conn.rollback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df6dc4",
   "metadata": {},
   "source": [
    "### Inserting 311 Complaint Data\n",
    "\n",
    "This code processes data from `geodf_311_data` and inserts it into the 'complaints' table in the database. It logs progress for each row or at specific intervals and commits the changes after processing the data. In case of a database error, it rolls back the transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac85091",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Process geodf_311_data\n",
    "    for index, row in geodf_311_data.iterrows():\n",
    "        geom_geojson = json.dumps(mapping(row['geometry'])) if row['geometry'] else None\n",
    "        data = {\n",
    "            'created_date': row['created_date'].isoformat() if pd.notnull(row['created_date']) else None,\n",
    "            'complaint_type': row['complaint_type'],\n",
    "            'zipcode': row['zipcode'],\n",
    "            'geometry': geom_geojson\n",
    "        }\n",
    "        insert_data(cursor, 'complaints', data, conn)\n",
    "\n",
    "        # Log progress for each row or at specific intervals\n",
    "        if (index + 1) % 100000 == 0 or (index + 1) == geodf_311_data.shape[0]:\n",
    "            logging.info(f\"Inserted Complaint row {index + 1} of {geodf_311_data.shape[0]}\")\n",
    "\n",
    "    # Commit the changes after processing geodf_311_data\n",
    "    conn.commit()\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(\"Database error:\", e)\n",
    "    conn.rollback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closes the database connection\n",
    "if conn is not None:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    print(\"Data processing and insertion completed successfully. Database connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc3383",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Understanding Data\n",
    "\n",
    "In this part, a series of SQL queries are crafted to gain insights into the datasets used for this project. Each query is designed to answer a specific question related to the data. The queries are saved as separate .sql files with descriptive names indicating their purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47194c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to a file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(outfile, 'w') as file:\n",
    "        file.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb27c1",
   "metadata": {},
   "source": [
    "## Query 1: Identifying Calm Areas to Live\n",
    "\n",
    "The purpose of this code block is to execute a SQL query to find the number of 311 complaints per zip code in New York City between October 1st, 2022, and September 30th, 2023, inclusive. The query results will be ordered by the number of complaints in descending order. The information can help identify areas that might be more calm to live in based on complaint counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb89f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = QUERY_DIR / \"complaints_per_zipcode.sql\"\n",
    "\n",
    "# SQL Query\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT zipcode, COUNT(*) AS num_complaints\n",
    "FROM complaints\n",
    "WHERE created_date BETWEEN '2022-10-01' AND '2023-09-30'\n",
    "GROUP BY zipcode\n",
    "ORDER BY num_complaints DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd893d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a DataFrame\n",
    "engine = create_engine(\"postgresql+psycopg2://joyliu@localhost:5432/group19project\")\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_1))\n",
    "    df1 = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743c037",
   "metadata": {},
   "source": [
    "## Query 2: Identifying Areas with the Most Greenery\n",
    "\n",
    "The purpose of this code block is to execute a SQL query to find the top 10 zip codes with the most trees in New York City, using data from the 'trees' table. The query results will be sorted by the total number of trees in descending order, helping to identify areas with the most greenery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a DataFrame\n",
    "engine = create_engine(\"postgresql+psycopg2://joyliu@localhost:5432/group19project\")\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_2))\n",
    "    df2 = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17416cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32340a2b",
   "metadata": {},
   "source": [
    "## Query 3: Affordability in Green Areas\n",
    "\n",
    "The purpose of this code block is to execute a SQL query to determine the average rent for August 2023 in the top 10 zip codes with the most trees in New York City. This query involves a JOIN operation between the 'trees' and 'rent_history' tables. The results are formatted for readability, showing zip codes and average rents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = QUERY_DIR / \"average_rent_in_greenest_areas.sql\"\n",
    "\n",
    "# SQL Query\n",
    "QUERY_3 = \"\"\"\n",
    "WITH TopTreeZipCodes AS (\n",
    "    SELECT zipcode\n",
    "    FROM trees\n",
    "    GROUP BY zipcode\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT r.zipcode, TO_CHAR(AVG(r.value), 'FM9,999,999.00') as average_rent\n",
    "FROM rent_history r\n",
    "INNER JOIN TopTreeZipCodes ttzc ON r.zipcode = ttzc.zipcode\n",
    "WHERE DATE_PART('month', r.date) = 8 AND DATE_PART('year', r.date) = 2023\n",
    "GROUP BY r.zipcode\n",
    "ORDER BY AVG(r.value) DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f255db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a DataFrame\n",
    "engine = create_engine(\"postgresql+psycopg2://joyliu@localhost:5432/group19project\")\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_3))\n",
    "    df3 = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f07e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ddcb",
   "metadata": {},
   "source": [
    "## Query 4: Rent, Trees, and Complaints Correlation\n",
    "\n",
    "This code block executes a SQL query to explore potential correlations between average rent, the number of trees, and the number of 311 complaints for zip codes in New York City. Specifically, it retrieves data for the month of January 2023. It can help identify patterns and correlations that may inform urban planning decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = QUERY_DIR / \"rent_trees_complaints_correlation.sql\"\n",
    "\n",
    "# SQL Query\n",
    "QUERY_4 = \"\"\"\n",
    "WITH RentData AS (\n",
    "    SELECT zipcode, AVG(value) as average_rent\n",
    "    FROM rent_history\n",
    "    WHERE DATE_PART('month', date) = 1 AND DATE_PART('year', date) = 2023\n",
    "    GROUP BY zipcode\n",
    "),\n",
    "RankedRent AS (\n",
    "    SELECT zipcode, average_rent, \n",
    "           DENSE_RANK() OVER (ORDER BY average_rent DESC) as high_rank,\n",
    "           DENSE_RANK() OVER (ORDER BY average_rent) as low_rank\n",
    "    FROM RentData\n",
    ")\n",
    "SELECT r.zipcode, TO_CHAR(r.average_rent, 'FM9,999,999.00') as average_rent, \n",
    "       COUNT(t.tree_id) as tree_count, COUNT(c.id) as complaint_count\n",
    "FROM RankedRent r\n",
    "LEFT JOIN trees t ON r.zipcode = t.zipcode\n",
    "LEFT JOIN complaints c ON r.zipcode = c.zipcode\n",
    "WHERE r.high_rank <= 5 OR r.low_rank <= 5\n",
    "GROUP BY r.zipcode, r.average_rent\n",
    "ORDER BY r.average_rent DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a DataFrame\n",
    "engine = create_engine(\"postgresql+psycopg2://joyliu@localhost:5432/group19project\")\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_4))\n",
    "    df4 = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9474d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a308b",
   "metadata": {},
   "source": [
    "## Query 5: Greenery Spatial Join\n",
    "\n",
    "This code block performs a SQL query to determine the number of trees in each zip code area using a spatial join. It joins both the `trees` table and the `zipcodes` table, where the coordinate point of each tree is inside the polygon boundary of the zip code as defined in the `zipcodes` table. The query aims to obtain results that match the results of Query 2. This spatial join approach considers the geographical boundaries of zip codes and the locations of individual trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = QUERY_DIR / \"greenery_spatial_join.sql\"\n",
    "\n",
    "# SQL Query\n",
    "QUERY_5 = \"\"\"\n",
    "SELECT z.zipcode, COUNT(t.tree_id) as tree_count\n",
    "FROM zip_codes z\n",
    "JOIN trees t ON z.zipcode = t.zipcode\n",
    "WHERE ST_Contains(z.geometry, t.geometry)\n",
    "GROUP BY z.zipcode\n",
    "ORDER BY tree_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77843b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a DataFrame\n",
    "engine = create_engine(\"postgresql+psycopg2://joyliu@localhost:5432/group19project\")\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_5))\n",
    "    df5 = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f59492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c079124",
   "metadata": {},
   "source": [
    "## Query 6: Trees Near Coordinate\n",
    "\n",
    "This code block executes a SQL query to find trees within a ½ mile radius of a specific coordinate pair: Latitude 40.80737875669467 and Longitude -73.96253174434912. The query returns information about each tree, including its ID, species, health, status, and coordinate location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = QUERY_DIR / \"trees_near_coordinate.sql\"\n",
    "\n",
    "# SQL Query\n",
    "QUERY_6 = \"\"\"\n",
    "SELECT t.tree_id, t.species, t.health, t.status, ST_AsText(t.geometry) as location\n",
    "FROM trees t\n",
    "WHERE ST_DWithin(\n",
    "    t.geometry, \n",
    "    ST_GeomFromText('POINT(-73.96253174434912 40.80737875669467)', 4326),\n",
    "    0.5 * 1609.34  -- 0.5 miles in meters\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query and store the results in a DataFrame\n",
    "engine = create_engine(\"postgresql+psycopg2://joyliu@localhost:5432/group19project\")\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(QUERY_6))\n",
    "    df6 = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query to a file\n",
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c72029",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Visualizing Data\n",
    "In Part 4, we focus on creating visualizations to gain insights and enhance our understanding of the datasets. These visualizations are generated using libraries like Matplotlib, Geopandas, and other visualization tools. Each visualization is designed to be self-explanatory, with labeled axes, titles, and appropriate visual elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49056f4e",
   "metadata": {},
   "source": [
    "## Visualization 1: Complaint Types Over Time\n",
    "\n",
    "**Objective:** Visualize the top 3 complaint types in New York City between October 1st, 2022, and September 30th, 2023, and track the number of complaints per day for these types.\n",
    "\n",
    "**Implementation:** \n",
    "1. We begin by querying the database to identify the top 3 complaint types during the specified timeframe.\n",
    "2. Next, we retrieve the daily counts of complaints for each of these top complaint types.\n",
    "3. Using Matplotlib, we create a line plot that illustrates the daily number of complaints over time for each of the top 3 complaint types.\n",
    "4. The x-axis represents the date, and the y-axis represents the number of complaints.\n",
    "5. Each complaint type is color-coded and labeled for clarity.\n",
    "6. The resulting visualization provides insights into the most prevalent issues in the city and how they vary over time.\n",
    "\n",
    "**Interpretation:** \n",
    "This visualization allows us to understand the frequency and patterns of the top 3 complaint types in NYC over the specified period. It can help city officials and residents monitor and address common concerns more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_3_complaints():\n",
    "    engine = create_engine(\"postgresql+psycopg2://cecilialin@localhost:5432/group19project\")\n",
    "\n",
    "    # Query to get top 3 complaint types\n",
    "    top_complaints_query = \"\"\"\n",
    "    SELECT complaint_type, COUNT(*) as total_complaints\n",
    "    FROM complaints\n",
    "    WHERE created_date BETWEEN '2022-10-01' AND '2023-09-30'\n",
    "    GROUP BY complaint_type\n",
    "    ORDER BY total_complaints DESC\n",
    "    LIMIT 3\n",
    "    \"\"\"\n",
    "    top_complaints_df = pd.read_sql(top_complaints_query, engine)\n",
    "\n",
    "    # Query to get daily complaints for these types\n",
    "    daily_complaints_query = \"\"\"\n",
    "    SELECT date_trunc('day', created_date) as date, complaint_type, COUNT(*) as daily_count\n",
    "    FROM complaints\n",
    "    WHERE created_date BETWEEN '2022-10-01' AND '2023-09-30'\n",
    "    AND complaint_type IN ('{}')\n",
    "    GROUP BY date, complaint_type\n",
    "    ORDER BY date\n",
    "    \"\"\".format(\"', '\".join(top_complaints_df['complaint_type'].tolist()))\n",
    "    daily_complaints_df = pd.read_sql(daily_complaints_query, engine)\n",
    "\n",
    "    # Plotting\n",
    "    for complaint_type in daily_complaints_df['complaint_type'].unique():\n",
    "        subset = daily_complaints_df[daily_complaints_df['complaint_type'] == complaint_type]\n",
    "        plt.plot(subset['date'], subset['daily_count'], label=complaint_type)\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Complaints')\n",
    "    plt.title('Top 3 Complaint Types in NYC')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_top_3_complaints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080f4a2c",
   "metadata": {},
   "source": [
    "## Visualization 2: Common Complaints in Zip Code 10027\n",
    "\n",
    "**Objective:** Visualize the top 10 complaint types in Zip Code 10027 between October 1st, 2018, and September 30th, 2023.\n",
    "\n",
    "**Implementation:** \n",
    "1. We use an SQL query to extract the top 10 complaint types from Zip Code 10027 within the specified timeframe.\n",
    "2. The retrieved data is then plotted as a bar chart using Matplotlib.\n",
    "3. The x-axis displays the complaint types, while the y-axis represents the number of complaints.\n",
    "4. The visualization provides a clear overview of the most common complaints in this specific zip code.\n",
    "5. To enhance readability, we rotate the x-axis labels.\n",
    "\n",
    "**Interpretation:** \n",
    "This visualization helps us understand the most frequent issues reported in Zip Code 10027 over the specified period. It can be valuable for local authorities and residents in addressing and prioritizing neighborhood concerns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_common_complaints_zip_10027():\n",
    "    engine = create_engine(\"postgresql+psycopg2://cecilialin@localhost:5432/group19project\")\n",
    "\n",
    "    # SQL query to get top 10 complaints in zip code 10027\n",
    "    query = \"\"\"\n",
    "    SELECT complaint_type, COUNT(*) as complaint_count\n",
    "    FROM complaints\n",
    "    WHERE zipcode = '10027' AND created_date BETWEEN '2018-10-01' AND '2023-09-30'\n",
    "    GROUP BY complaint_type\n",
    "    ORDER BY complaint_count DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df.plot(kind='bar', x='complaint_type', y='complaint_count', legend=False, ax=plt.gca())\n",
    "    plt.title('Top 10 Complaints in Zip Code 10027 (2018-2023)')\n",
    "    plt.xlabel('Complaint Type')\n",
    "    plt.ylabel('Number of Complaints')\n",
    "    plt.xticks(rotation=60)  # Rotate labels\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5551ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_common_complaints_zip_10027()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93f1a0",
   "metadata": {},
   "source": [
    "## Visualization 3: Correlation Between Rent, Trees, and Complaints\n",
    "\n",
    "**Objective:** Visualize the correlation between average rent, the number of trees, and the number of complaints by zip code from January 1st, 2015, to September 30th, 2023.\n",
    "\n",
    "**Implementation:** \n",
    "1. We use an SQL query to retrieve data on average rent, the number of trees, and the number of complaints for each zip code within the specified timeframe.\n",
    "2. The retrieved data is plotted using Matplotlib in two subplots:\n",
    "   - The first subplot shows a scatter plot of average rent vs. the number of trees by zip code.\n",
    "   - The second subplot displays a scatter plot of average rent vs. the number of complaints by zip code.\n",
    "3. Both subplots share the x-axis, allowing for easy comparison.\n",
    "4. This visualization helps explore potential correlations between these factors.\n",
    "\n",
    "**Interpretation:** \n",
    "By examining the two scatter plots, we can gain insights into any potential relationships between rent, the presence of trees, and the number of complaints in different zip codes. This analysis can assist in understanding how these variables might be related and their implications for specific areas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f873788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rent_trees_complaints_correlation():\n",
    "    engine = create_engine(\"postgresql+psycopg2://cecilialin@localhost:5432/group19project\")\n",
    "\n",
    "    # SQL query to get rent, tree count, and complaint count by zip code\n",
    "    query = \"\"\"\n",
    "    SELECT r.zipcode, AVG(r.value) as average_rent, \n",
    "           COUNT(DISTINCT t.tree_id) as tree_count, \n",
    "           COUNT(DISTINCT c.id) as complaint_count\n",
    "    FROM rent_history r\n",
    "    LEFT JOIN trees t ON r.zipcode = t.zipcode\n",
    "    LEFT JOIN complaints c ON r.zipcode = c.zipcode\n",
    "    WHERE r.date BETWEEN '2015-01-01' AND '2023-09-30'\n",
    "    GROUP BY r.zipcode\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "\n",
    "    # Creating subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    # First subplot: Rent vs. Number of Trees\n",
    "    ax1.scatter(df['average_rent'], df['tree_count'])\n",
    "    ax1.set_title('Rent vs. Number of Trees by Zip Code')\n",
    "    ax1.set_ylabel('Number of Trees')\n",
    "\n",
    "    # Second subplot: Rent vs. Number of Complaints\n",
    "    ax2.scatter(df['average_rent'], df['complaint_count'])\n",
    "    ax2.set_title('Rent vs. Number of Complaints by Zip Code')\n",
    "    ax2.set_xlabel('Average Rent')\n",
    "    ax2.set_ylabel('Number of Complaints')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d65a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rent_trees_complaints_correlation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891bbd4b",
   "metadata": {},
   "source": [
    "## Visualization 4: Relationship Between Rent and Complaints\n",
    "\n",
    "**Objective:** Visualize the relationship between average rent and the number of 311 complaints in different rent ranges for September 2023.\n",
    "\n",
    "**Implementation:** \n",
    "1. An SQL query is used to retrieve data on average rent and the number of complaints for each zip code within the specified timeframe (September 2023).\n",
    "2. The data is grouped into bins of $1000 increments to categorize average rent ranges.\n",
    "3. A boxplot is created to display the distribution of the number of complaints within each rent range.\n",
    "4. The x-axis represents the binned average rent, and the y-axis shows the number of complaints.\n",
    "5. This visualization helps determine whether areas with higher rent levels tend to have fewer 311 complaints.\n",
    "\n",
    "**Interpretation:** \n",
    "The boxplot provides insights into the relationship between average rent and the number of complaints in September 2023. By categorizing average rent into bins and visualizing the spread of complaints within each range, we can identify trends or patterns in how rent levels may correlate with the frequency of complaints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rent_vs_complaints():\n",
    "    engine = create_engine(\"postgresql+psycopg2://cecilialin@localhost:5432/group19project\")\n",
    "\n",
    "    # SQL query for average rent and complaints\n",
    "    query = \"\"\"\n",
    "    SELECT r.zipcode, AVG(r.value) as average_rent, \n",
    "           COUNT(c.id) as complaint_count\n",
    "    FROM rent_history r\n",
    "    LEFT JOIN complaints c ON r.zipcode = c.zipcode\n",
    "    WHERE r.date BETWEEN '2022-10-01' AND '2023-09-30'\n",
    "    GROUP BY r.zipcode\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "\n",
    "    # Creating bins for rent\n",
    "    df['rent_bin'] = pd.cut(df['average_rent'], bins=range(0, int(max(df['average_rent'])) + 1000, 1000))\n",
    "\n",
    "    # Boxplot\n",
    "    df.boxplot(by='rent_bin', column='complaint_count', figsize=(10, 6))\n",
    "    plt.title('Rent vs Complaints in September 2023')\n",
    "    plt.xlabel('Average Rent (Binned)')\n",
    "    plt.ylabel('Number of Complaints')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8130bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_rent_vs_complaints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75145772",
   "metadata": {},
   "source": [
    "## Visualization 5: Recent 311 Incidents Near a Specific Location\n",
    "\n",
    "**Objective:** Create a geospatial plot of the coordinates of 311 incidents reported between January 1st, 2023, and September 30th, 2023, within a 1-kilometer radius of a given coordinate point.\n",
    "\n",
    "**Implementation:** \n",
    "1. An SQL query is used to retrieve 311 incident data within the specified timeframe (January to September 2023) and within a 1-kilometer radius of a predefined coordinate point.\n",
    "2. The data is converted into a GeoDataFrame for geospatial plotting.\n",
    "3. The resulting plot displays the location of 311 incidents as red markers on a map.\n",
    "4. The point of interest is defined by the coordinates (latitude: 40.80737875669467, longitude: -73.96253174434912).\n",
    "\n",
    "**Interpretation:** \n",
    "This visualization provides a geospatial representation of 311 incidents reported near a specific location. By plotting the incident coordinates on a map, we can visually assess the distribution and proximity of recent 311 incidents to the given point of interest. This information can help in understanding the localized trends and issues within the immediate area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f52d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_recent_311_incidents():\n",
    "    engine = create_engine(\"postgresql+psycopg2://cecilialin@localhost:5432/group19project\")\n",
    "\n",
    "    # Define the center point and radius (1 km)\n",
    "    center_point = Point(-73.96253174434912, 40.80737875669467)\n",
    "    radius = 1000  # in meters\n",
    "\n",
    "    # SQL query to get 311 incidents within the radius and timeframe\n",
    "    query = \"\"\"\n",
    "    SELECT id, ST_X(geometry) as longitude, ST_Y(geometry) as latitude\n",
    "    FROM complaints\n",
    "    WHERE created_date BETWEEN '2023-01-01' AND '2023-09-30'\n",
    "    AND ST_DWithin(geometry, ST_SetSRID(ST_MakePoint(%s, %s), 4326)::geography, %s)\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine, params=[center_point.x, center_point.y, radius])\n",
    "\n",
    "    # Convert DataFrame to GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    gdf.plot(ax=ax, marker='o', color='red', markersize=5)\n",
    "    plt.title('Recent 311 Incidents Near a Point')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5d323",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_recent_311_incidents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c1c58",
   "metadata": {},
   "source": [
    "## Visualization 6: Trees and New Tree Requests in NYC\n",
    "\n",
    "**Objective:** Create a geospatial plot that visualizes two sets of data - the coordinates of trees in NYC and the coordinates of \"New Tree Request\" 311 complaints made between October 1st, 2018, and September 30th, 2023.\n",
    "\n",
    "**Implementation:** \n",
    "1. Two separate SQL queries are used to retrieve data for trees and \"New Tree Request\" complaints from the database.\n",
    "2. The data is converted into GeoDataFrames to enable geospatial plotting.\n",
    "3. The GeoDataFrames are overlaid on a single plot, with trees represented as green markers and new tree requests as blue markers.\n",
    "4. The resulting plot displays the locations of both trees and new tree requests, allowing for a visual comparison.\n",
    "\n",
    "**Interpretation:** \n",
    "This visualization allows us to explore the relationship between existing trees and the demand for new trees in NYC. By plotting both datasets on a map, we can visually assess areas with a high concentration of existing trees and areas with a significant number of new tree requests. This information can be valuable for urban planning and tree planting initiatives to identify areas where tree planting efforts may be needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trees_and_new_requests():\n",
    "    engine = create_engine(\"postgresql+psycopg2://cecilialin@localhost:5432/group19project\")\n",
    "\n",
    "    # SQL query for trees\n",
    "    trees_query = \"SELECT tree_id, ST_X(geometry) as longitude, ST_Y(geometry) as latitude FROM trees\"\n",
    "    trees_df = pd.read_sql(trees_query, engine)\n",
    "\n",
    "    # SQL query for new tree requests\n",
    "    requests_query = \"\"\"\n",
    "    SELECT id, ST_X(geometry) as longitude, ST_Y(geometry) as latitude\n",
    "    FROM complaints\n",
    "    WHERE complaint_type = 'New Tree Request' AND created_date BETWEEN '2018-10-01' AND '2023-09-30'\n",
    "    \"\"\"\n",
    "    requests_df = pd.read_sql(requests_query, engine)\n",
    "\n",
    "    # Convert to GeoDataFrames\n",
    "    trees_gdf = gpd.GeoDataFrame(trees_df, geometry=gpd.points_from_xy(trees_df.longitude, trees_df.latitude))\n",
    "    requests_gdf = gpd.GeoDataFrame(requests_df, geometry=gpd.points_from_xy(requests_df.longitude, requests_df.latitude))\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    trees_gdf.plot(ax=ax, marker='o', color='green', markersize=5, label='Trees')\n",
    "    requests_gdf.plot(ax=ax, marker='x', color='blue', markersize=5, label='New Tree Requests')\n",
    "    plt.title('Trees and New Tree Requests in NYC')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trees_and_new_requests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
